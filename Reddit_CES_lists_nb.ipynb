{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.filelist import findall\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import demoji\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreiving posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/outdoors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(dt.datetime.timestamp(dt.datetime.strptime('2008-06-20 00:00:00', '%Y-%m-%d %H:%M:%S'))) #time Outdoors was created\n",
    "end_time = int(dt.datetime.timestamp(dt.datetime.strptime('2022-08-01 00:00:00', '%Y-%m-%d %H:%M:%S')))\n",
    "current_time = int(dt.datetime.timestamp(dt.datetime.now()))\n",
    "\n",
    "# Create string specifying time frame that can be used for file name when saving data as csv\n",
    "search_time = '20080620-20220816' \n",
    "\n",
    "# Set subreddit and limit\n",
    "subreddit = 'Outdoors'\n",
    "limit = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/earthporn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(dt.datetime.timestamp(dt.datetime.strptime('2011-02-14 00:00:00', '%Y-%m-%d %H:%M:%S'))) #time EarthPorn was created\n",
    "end_time = int(dt.datetime.timestamp(dt.datetime.strptime('2022-08-01 00:00:00', '%Y-%m-%d %H:%M:%S')))\n",
    "current_time = int(dt.datetime.timestamp(dt.datetime.now()))\n",
    "\n",
    "# Create string specifying time frame that can be used for file name when saving data as csv\n",
    "search_time = '20110214-20220816' \n",
    "\n",
    "# Set subreddit and limit\n",
    "subreddit = 'EarthPorn'\n",
    "limit = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query posts from pushshift using search_submissions with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters:  \n",
    "max_ids_per_request = 500 (max)  \n",
    "max_results_per_request = 100 (max)  \n",
    "mem_safe = False -> stores responses in cache during operation if True  \n",
    "safe_exit = False -> will safely exit if interupted by storing current responses and requests in the cache if True  \n",
    "cache_dir -> path to cache responses in when mem_safe or safe_exit is enabled  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = api.search_submissions(subreddit=subreddit, limit=limit, before=current_time, after=start_time)\n",
    "print(f'Retrieved {len(posts)} posts from Pushshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data frame for posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list = [post for post in posts]\n",
    "posts_df = pd.DataFrame(post_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview sample of posts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', 25)\n",
    "print(posts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all column names and remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.drop(columns = ['author_flair_css_class', 'author_flair_text'], inplace = True)\n",
    "\n",
    "#posts_df.drop(columns = ['author_flair_css_class', 'author_flair_text', 'gilded', 'mod_reports', 'user_reports', 'brand_safe', 'contest_mode', 'spoiler', 'suggested_sort',\n",
    "#                         'author_flair_richtext', 'author_flair_type', 'can_mod_post', 'link_flair_richtext', 'link_flair_text_color', 'link_flair_type', 'rte_mode',\n",
    "#                         'subreddit_type', 'thumbnail_height', 'thumbnail_width', 'author_flair_background_color', 'author_flair_text_color', 'author_patreon_flair', 'gildings', \n",
    "#                         'is_robot_indexable', 'link_flair_background_color', 'send_replies', 'no_follow', 'updated_utc', 'all_awardings', 'allow_live_comments', 'author_premium',\n",
    "#                         'awarders', 'total_awards_received', 'treatment_tags', 'is_created_from_ads_ui', 'parent_whitelist_status', 'pwls', 'url_overridden_by_dest',\n",
    "#                         'whitelist_status', 'wls', 'removed_by_category', 'author_is_blocked', 'approved_at_utc', 'banned_at_utc', 'steward_reports', 'removed_by', 'poll_data',\n",
    "#                         'top_awarded_type', 'retrieved_on'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change column names and reorder columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dictionary - 'old name' : 'new name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {'id' : 'PostID',\n",
    "                'subreddit' : 'Subreddit',\n",
    "                'subreddit_id' : 'SubredditID',\n",
    "                'created_utc' : 'PostTime',\n",
    "                'title' : 'PostTitle',\n",
    "                'author' : 'Username',\n",
    "                'author_created_utc' : 'UserCreatedTime',\n",
    "                'author_fullname' : 'AuthorName', \n",
    "                'domain' : 'ImageDomain',\n",
    "                'full_link' : 'Link',\n",
    "                'is_self' : 'IsTextPost',\n",
    "                'media_embed' : 'EmbeddedMedia',\n",
    "                'secure_media_embed' : 'SecureEmbeddedMedia',\n",
    "                'num_comments' : 'CommentNumber', \n",
    "                'over_18' : 'NSFW',\n",
    "                'permalink' : 'Permalink', \n",
    "                'score' : 'Upvotes', \n",
    "                'selftext' : 'PostText', \n",
    "                'thumbnail' : 'Thumbnail',\n",
    "                'url' : 'ImageURL',\n",
    "                'media' : 'Media',\n",
    "                'secure_media' : 'SecureMedia',\n",
    "                'stickied' : 'Stickied',\n",
    "                'locked' : 'CommentsLocked',\n",
    "                'post_hint' : 'PostHint',\n",
    "                'preview' : 'Preview',\n",
    "                'is_crosspostable' : 'IsCrosspostable',\n",
    "                'is_reddit_media_domain' : 'IsRedditMediaDomain',\n",
    "                'is_video' : 'IsVideo',\n",
    "                'num_crossposts' : 'CrosspostsNumber', \n",
    "                'pinned' : 'Pinned',\n",
    "                'crosspost_parent' : 'CrosspostParent',\n",
    "                'crosspost_parent_list' : 'CrosspostParentList',\n",
    "                'is_meta' : 'IsMeta',\n",
    "                'is_original_content' : 'IsOriginal',\n",
    "                'media_only' : 'OnlyMedia', \n",
    "                'subreddit_subscribers' : 'SubRedditSubscribers',\n",
    "                'media_metadata' : 'MediaMetadata', \n",
    "                'upvote_ratio' : 'UpvoteRatio', \n",
    "                'gallery_data' : 'GalleryData', \n",
    "                'is_gallery' : 'IsGallery', \n",
    "                'author_cakeday' : 'AuthorBirthdate',\n",
    "                'edited' : 'Edited', \n",
    "                'view_count' : 'ViewCount', \n",
    "                'author_id' : 'AuthorID',\n",
    "                'og_description' : 'OGDescription',\n",
    "                'og_title' : 'OGTitle'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Rename columns using dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_tidy_df = posts_df.rename(columns = column_names)\n",
    "# Check to see if columns have been renamed\n",
    "posts_tidy_df.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_tidy_df = posts_tidy_df[['Subreddit', 'SubredditID', 'PostTitle', 'PostID', 'PostTime', 'Username', 'Upvotes', 'CommentNumber', 'ImageDomain', 'ImageURL', 'UserCreatedTime',\n",
    "                                       'AuthorName', 'Permalink', 'Link', 'IsTextPost', 'PostText', 'EmbeddedMedia', 'Thumbnail', 'NSFW']]\n",
    "                                       \n",
    "\n",
    "#posts_reordered_df = posts_renamed_df[['Subreddit', 'SubredditID', 'PostTitle', 'PostID', 'PostTime', 'Username', 'ViewCount', 'Upvotes', 'UpvoteRatio', 'CommentNumber', 'Edited', 'OGDescription',\n",
    "#                      'OGTitle', 'ImageDomain', 'ImageURL', 'Permalink', 'Link', 'IsTextPost', 'PostText', 'UserCreatedTime', 'AuthorID', 'AuthorName', 'AuthorBirthdate', 'IsVideo', \n",
    "#                     'IsMeta', 'IsOriginal', 'IsGallery', 'GalleryData', 'IsRedditMediaDomain', 'IsCrosspostable', 'CrosspostsNumber', 'CrosspostParent', 'CrosspostParentList',\n",
    "#                     'SubRedditSubscribers', 'OnlyMedia', 'MediaMetadata', 'EmbeddedMedia', 'SecureEmbeddedMedia', 'Media', 'SecureMedia', 'Thumbnail', 'Stickied', 'Pinned',\n",
    "#                     'PostHint', 'Preview', 'CommentsLocked', 'NSFW']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert time stamp from UNIX to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_tidy_df['PostTime'] = pd.to_datetime(posts_tidy_df['PostTime'], utc=True, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "posts_tidy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data frame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Reddit_'+ subreddit + '_' + search_time + '.csv'\n",
    "posts_tidy_df.to_csv(filename, header=True, index=False, columns=list(posts_tidy_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'Earthporn' # or 'Outdoors'\n",
    "posts_tidy_filename = 'Reddit_'+ subreddit + '_' + search_time + '.csv'\n",
    "posts_tidy_df = pd.read_csv(posts_tidy_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove brackets and other characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_clean_df = posts_tidy_df.rename(columns = column_names)\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"\\[(.*?)\\]\", value=\"\", regex=True, inplace=True) \n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"\\(\\d*?\\s*[\\u00D7?x?]\\s*\\d*?\\)\", value=\"\", regex=True, inplace=True)\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"\\(\", value=\"\", regex=True, inplace=True)\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"\\)\", value=\"\", regex=True, inplace=True)\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"-\", value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_clean_df['PostTitle'].replace(to_replace=\"\\#\", value=\"\", regex=True, inplace=True) #only removes # and keeps #text\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"(\\#\\w*\\b?)\", value=\"\", regex=True, inplace=True) #removes # and #text\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"@\", value=\"\", regex=True, inplace=True) #removes @ and @text\n",
    "posts_clean_df['PostTitle'].replace(to_replace=\"(@\\w*\\b?)\", value=\"\", regex=True, inplace=True) #removes @ and @text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace emojis with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji.findall(posts_clean_df['PostTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_udpipe.download(\"en\")\n",
    "nlp = spacy_udpipe.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new data frame for annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Sentence', 'Text ID', 'IDX', 'Text', 'Lemma', 'POS', 'Form', 'Dependency', 'Sentiment'] \n",
    "posts_annotated_df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create empty lists to store token values in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "i = []\n",
    "idx = []\n",
    "word = []\n",
    "lemma = []\n",
    "pos = []\n",
    "tag = []\n",
    "dep = []\n",
    "sentiment = []\n",
    "form = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in posts_clean_df.iterrows():\n",
    "    text = row['PostTitle']\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        #print('Sentence:' + token.sent)\n",
    "        sent.append(token.sent)\n",
    "        i.append(token.i)\n",
    "        idx.append(token.idx)\n",
    "        word.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        form.append(token.morph.get(\"VerbForm\"))\n",
    "        tag.append(token.tag_)\n",
    "        dep.append(token.dep_)\n",
    "        sentiment.append(token.sentiment)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add token annotations to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_annotated_df['Sentence'] = sent\n",
    "posts_annotated_df['Text ID'] = i\n",
    "posts_annotated_df['Text'] = word\n",
    "posts_annotated_df['Lemma'] = lemma\n",
    "posts_annotated_df['POS'] = pos\n",
    "posts_annotated_df['VerbForm'] = form\n",
    "posts_annotated_df['Dependency'] = dep\n",
    "posts_annotated_df['IDX'] = idx\n",
    "posts_annotated_df['Sentiment'] = sentiment\n",
    "posts_annotated_df['VerbForm'] = posts_annotated_df['VerbForm'].str[0]\n",
    "\n",
    "print(posts_annotated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save annotations as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Reddit_Annotated_'+ subreddit + '_' + search_time + '.csv'\n",
    "posts_annotated_df.to_csv(filename, header=True, index=False, columns=list(posts_annotated_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create activity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load r/outdoors data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'Outdoors'\n",
    "reddit_actions_filename = 'Reddit_Annotated_'+ subreddit + '_' + search_time + '.csv'\n",
    "outdoors_actions = pd.read_csv(reddit_actions_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for non-finite verbs and principle verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_df = outdoors_actions[outdoors_actions['POS'].str.contains(\"VERB\")]\n",
    "actions_df = verbs_df[verbs_df['VerbForm'].str.contains('Ger', na = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list with actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_list = actions_df['Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data frame with actions and subreddit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CES_actions = pd.DataFrame()\n",
    "CES_actions['Action'] = actions_list\n",
    "CES_actions['Subreddit'] = subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save actions as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_filename = 'CES_actions_Reddit' + '.csv'\n",
    "CES_actions.to_csv(actions_filename, header=True, index=False, columns=list(CES_actions.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load r/earthporn data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\acali\\Documents\\GitHub\\SoMe-CES\\Reddit_activity_lists_nb.ipynb Cell 71\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acali/Documents/GitHub/SoMe-CES/Reddit_activity_lists_nb.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m subreddit \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mEarthporn\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/acali/Documents/GitHub/SoMe-CES/Reddit_activity_lists_nb.ipynb#Y153sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mReddit_Annotated_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m subreddit \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m search_time \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acali/Documents/GitHub/SoMe-CES/Reddit_activity_lists_nb.ipynb#Y153sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m earthporn_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(features_filename)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_time' is not defined"
     ]
    }
   ],
   "source": [
    "subreddit = 'Earthporn'\n",
    "features_filename = 'Reddit_Annotated_'+ subreddit + '_' + search_time + '.csv'\n",
    "earthporn_features = pd.read_csv(features_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_df = earthporn_features[earthporn_features['POS'].str.contains(\"NOUN|PROPNOUN\")]\n",
    "features_df = nouns_df[nouns_df['Dependency'].str.contains('ROOT')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = features_df['Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data frame with features and subreddit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CES_features = pd.DataFrame()\n",
    "CES_features['Features'] = features_list\n",
    "CES_features['Subreddit'] = subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filename = 'CES_features_Reddit.csv'\n",
    "CES_features.to_csv(features_filename, header=True, index=False, columns=list(CES_features.axes[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "654f3540e6af9084947d0df77248ac19dd4be50e477a3459f2fef29575e02c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
