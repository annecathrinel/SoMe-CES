{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(academictwitteR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Sys.setenv(TWITTER_BEARER = 'AAAAAAAAAAAAAAAAAAAAAGLXewEAAAAAN0VIz6Op9kTBnrE3%2FFzC94k7WbU%3D6Vd6oraH6qMkuxpQX62SBr0yLezYWXiUFLEmshgT6Al1o8Ory8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_bearer() # it checks out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CES actions, features, and species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.actions_Outdoors_reddit.Rdata\")\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.asset_EarthPorn_IUCN_reddit.Rdata\")\n",
    "\n",
    "CES.features = CES.asset[1:56]\n",
    "CES.animals = CES.asset[57:437]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run only to get complementary lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Getting list of actions not included in original sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map/CES.actions_Outdoors_reddit.Rdata\")\n",
    "CES.actions.old <- CES.actions\n",
    "\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.actions_Outdoors_reddit.Rdata\")\n",
    "\n",
    "CES.actions.complement <- CES.actions[!names(CES.actions)%in%names(CES.actions.old)]\n",
    "\n",
    "CES.actions <- CES.actions.complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(CES.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "start = \"2022-12-21T00:00:00Z\"\n",
    "end = \"2022-12-31T23:59:59Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "setwd(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/2022_dec_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "path = paste0(\"CES_raw/\", names(CES.actions)[i], \"/\")\n",
    "query_term = paste(names(CES.actions)[i], CES.features, sep = \" \")\n",
    "\n",
    "if (sum(nchar(query_term))<752) {\n",
    "tweets =\n",
    "get_all_tweets(\n",
    "\tquery = query_term,\n",
    "\tstart,\n",
    "\tend,\n",
    "\tn = 1000000, #for trials restrict collection\n",
    "\tdata_path = path,\n",
    "\tis_retweet = FALSE,\n",
    "\tis_reply = FALSE,\n",
    "\tis_quote = FALSE,\n",
    "\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    ")\n",
    "save(tweets,file = paste0(\"CES_df/\", names(CES.actions)[i], \".Rdata\"))\n",
    "rm(tweets)\n",
    "} else {\n",
    "query_term = paste(names(CES.actions)[i], CES.features[1:20], sep=\" \")\n",
    "path = paste0(\"CES_raw/\", names(CES.actions)[i], \"1/\")\n",
    "tweets1 =\n",
    "get_all_tweets(\n",
    "\tquery = query_term,\n",
    "\tstart,\n",
    "\tend,\n",
    "\tn = 1000000, #for trials restrict collection\n",
    "\tdata_path = path,\n",
    "\tis_retweet = FALSE,\n",
    "\tis_reply = FALSE,\n",
    "\tis_quote = FALSE,\n",
    "\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    ")\n",
    "\n",
    "query_term = paste(names(CES.actions)[i], CES.features[21:40], sep=\" \")\n",
    "path = paste0(\"CES_raw/\", names(CES.actions)[i], \"2/\")\n",
    "tweets2 =\n",
    "get_all_tweets(\n",
    "\tquery = query_term,\n",
    "\tstart,\n",
    "\tend,\n",
    "\tn = 1000000, #for trials restrict collection\n",
    "\tdata_path = path,\n",
    "\tis_retweet = FALSE,\n",
    "\tis_reply = FALSE,\n",
    "\tis_quote = FALSE,\n",
    "\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    ")\n",
    "\n",
    "query_term = paste(names(CES.actions)[i], CES.features[41:56], sep=\" \")\n",
    "path = paste0(\"CES_raw/\", names(CES.actions)[i], \"3/\")\n",
    "tweets3 =\n",
    "get_all_tweets(\n",
    "\tquery = query_term,\n",
    "\tstart,\n",
    "\tend,\n",
    "\tn = 1000000, #for trials restrict collection\n",
    "\tdata_path = path,\n",
    "\tis_retweet = FALSE,\n",
    "\tis_reply = FALSE,\n",
    "\tis_quote = FALSE,\n",
    "\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    ")\n",
    "\n",
    "tweets = list(tweets1, tweets2, tweets3)\n",
    "save(tweets, file = paste0(\"CES_df/\", names(CES.actions)[i], \".Rdata\"))\n",
    "rm(tweets1, tweets2, tweets3, tweets)\n",
    "}\n",
    "print(i)\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For entire activity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for (i in 101:length(names(CES.actions))) {\n",
    "\tpath = paste0(\"CES_raw/\", names(CES.actions)[i], \"/\")\n",
    "\tquery_term = paste(names(CES.actions)[i], CES.features, sep = \" \")\n",
    "\tif (sum(nchar(query_term))<752) {\n",
    "\t\ttweets =\n",
    "\t\t  get_all_tweets(\n",
    "\t\t\tquery = query_term,\n",
    "\t\t\tstart,\n",
    "\t\t\tend,\n",
    "\t\t\tn = 1000000, #for trials restrict collection\n",
    "\t\t\tdata_path = path,\n",
    "\t\t\tis_retweet = FALSE,\n",
    "\t\t\tis_reply = FALSE,\n",
    "\t\t\tis_quote = FALSE,\n",
    "\t\t\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    "\t\t)\n",
    "\t\tsave(tweets,file = paste0(\"CES_df/\", names(CES.actions)[i], \".Rdata\"))\n",
    "\t\trm(tweets)\n",
    "\t} else {\n",
    "\t\tquery_term = paste(names(CES.actions)[i], CES.features[1:20], sep=\" \")\n",
    "\t\tpath = paste0(\"CES_raw/\", names(CES.actions)[i], \"1/\")\n",
    "\t\ttweets1 =\n",
    "\t\t  get_all_tweets(\n",
    "\t\t\tquery = query_term,\n",
    "\t\t\tstart,\n",
    "\t\t\tend,\n",
    "\t\t\tn = 1000000, #for trials restrict collection\n",
    "\t\t\tdata_path = path,\n",
    "\t\t\tis_retweet = FALSE,\n",
    "\t\t\tis_reply = FALSE,\n",
    "\t\t\tis_quote = FALSE,\n",
    "\t\t\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    "\t\t  )\n",
    "\t\tquery_term = paste(names(CES.actions)[i], CES.features[21:40], sep=\" \")\n",
    "\t\tpath = paste0(\"CES_raw/\", names(CES.actions)[i], \"2/\")\n",
    "\t\ttweets2 =\n",
    "\t\t  get_all_tweets(\n",
    "\t\t\tquery = query_term,\n",
    "\t\t\tstart,\n",
    "\t\t\tend,\n",
    "\t\t\tn = 1000000, #for trials restrict collection\n",
    "\t\t\tdata_path = path,\n",
    "\t\t\tis_retweet = FALSE,\n",
    "\t\t\tis_reply = FALSE,\n",
    "\t\t\tis_quote = FALSE,\n",
    "\t\t\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    "\t\t  )\n",
    "\t\tquery_term = paste(names(CES.actions)[i], CES.features[41:56], sep=\" \")\n",
    "\t\tpath = paste0(\"CES_raw/\", names(CES.actions)[i], \"3/\")\n",
    "\t\ttweets3 =\n",
    "\t\t  get_all_tweets(\n",
    "\t\t\tquery = query_term,\n",
    "\t\t\tstart,\n",
    "\t\t\tend,\n",
    "\t\t\tn = 1000000, #for trials restrict collection\n",
    "\t\t\tdata_path = path,\n",
    "\t\t\tis_retweet = FALSE,\n",
    "\t\t\tis_reply = FALSE,\n",
    "\t\t\tis_quote = FALSE,\n",
    "\t\t\tremove_promoted = TRUE #these ensure we are only collecting original tweets\n",
    "\t\t  )\n",
    "\t\ttweets = list(tweets1, tweets2, tweets3)\n",
    "\t\tsave(tweets, file = paste0(\"CES_df/\", names(CES.actions)[i], \".Rdata\"))\n",
    "\t\trm(tweets1, tweets2, tweets3, tweets)\n",
    "\t}\n",
    "\t\tprint(i)\n",
    "\t\tflush.console()\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019 + 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "files <- list.files(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/2022/CES_df\", full.names=TRUE)\n",
    "file.nm <- list.files(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation ACL/2022/CES_df\")\n",
    "file.nm <- gsub(\".Rdata\",\"\",file.nm)\n",
    "file.nm <- gsub(\"\\\\d\",\"\",file.nm)\n",
    "\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.actions_Outdoors_reddit.Rdata\")\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.asset_EarthPorn_IUCN_reddit.Rdata\")\n",
    "\n",
    "CES.features<-CES.asset[1:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(length(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "load(files[i])\n",
    "str(tweets[[1]], vec.len = 0)\n",
    "\n",
    "#Create data frame with select columns\n",
    "tweet.detail.2022 = data.frame(action = file.nm[i], id = tweets$id, author_id = tweets$author_id, conversation_id = tweets$conversation_id, datetime = tweets$created_at, text = tweets$text, lang = tweets$lang)\n",
    "\n",
    "#Add feature column\n",
    "feat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets$text, ignore.case = TRUE))\n",
    "tweet.detail.2022$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "tweet.detail.2022$annotation_type = unlist(lapply(tweets$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "tweet.detail.2022$annotation_normalized_text = unlist(lapply(tweets$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "tweet.detail.2022<-cbind(tweet.detail.2022, tweets$public_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for (i in 210:length(files)) {\n",
    "\tload(files[i])\n",
    "\tif(length(tweets) == 2) {\n",
    "\t\ttweets1 = tweets[[1]]\n",
    "\t\ttemp1 = data.frame(action = file.nm[i], id = tweets1$id, author_id = tweets1$author_id, conversation_id = tweets1$conversation_id, datetime = tweets1$created_at, text = tweets1$text, lang = tweets1$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets1$text, ignore.case = TRUE))\n",
    "\t  \ttemp1$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t \ttemp1$annotation_type = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t\ttemp1$annotation_context = unlist(lapply(tweets1$context$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1$annotation_normalized_text = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1 = cbind(temp1, tweets1$public_metrics)\n",
    "\t  \n",
    "\t  \ttweets2 = tweets[[2]]\n",
    "\t  \ttemp2 = data.frame(action = file.nm[i], id = tweets2$id, author_id = tweets2$author_id, conversation_id = tweets2$conversation_id, datetime = tweets2$created_at, text = tweets2$text, lang = tweets2$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets2$text, ignore.case = TRUE))\n",
    "\t  \ttemp2$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t  \ttemp2$annotation_type = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2$annotation_normalized_text = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2 = cbind(temp2, tweets2$public_metrics)\n",
    "\t  \n",
    "\t  \ttweet.detail.2022<-rbind(tweet.detail.2022, temp1)\n",
    " \t  \ttweet.detail.2022<-rbind(tweet.detail.2022, temp2)\n",
    " \t  \trm(temp1, temp2, tweets, tweets1, tweets2)\n",
    "\t\t}\n",
    "\telse if(length(tweets) == 3) {\n",
    "\t\ttweets1 = tweets[[1]]\n",
    "\t\ttemp1 = data.frame(action = file.nm[i], id = tweets1$id, author_id = tweets1$author_id, conversation_id = tweets1$conversation_id, datetime = tweets1$created_at, text = tweets1$text, lang = tweets1$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets1$text, ignore.case = TRUE))\n",
    "\t  \ttemp1$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t \ttemp1$annotation_type = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t\ttemp1$annotation_context = unlist(lapply(tweets1$context$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1$annotation_normalized_text = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1 = cbind(temp1, tweets1$public_metrics)\n",
    "\t  \n",
    "\t  \ttweets2 = tweets[[2]]\n",
    "\t  \ttemp2 = data.frame(action = file.nm[i], id = tweets2$id, author_id = tweets2$author_id, conversation_id = tweets2$conversation_id, datetime = tweets2$created_at, text = tweets2$text, lang = tweets2$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets2$text, ignore.case = TRUE))\n",
    "\t  \ttemp2$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t  \ttemp2$annotation_type = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2$annotation_normalized_text = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2 = cbind(temp2, tweets2$public_metrics)\n",
    "\n",
    "\t\ttweets3 = tweets[[3]]\n",
    "\t  \ttemp3 = data.frame(action = file.nm[i], id = tweets3$id, author_id = tweets3$author_id, conversation_id = tweets3$conversation_id, datetime = tweets3$created_at, text = tweets3$text, lang = tweets3$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets3$text, ignore.case = TRUE))\n",
    "\t  \ttemp3$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t  \ttemp3$annotation_type = unlist(lapply(tweets3$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp3$annotation_normalized_text = unlist(lapply(tweets3$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp3 = cbind(temp3, tweets3$public_metrics)\n",
    "\t  \n",
    "\t  \ttweet.detail.2022<-rbind(tweet.detail.2022, temp1)\n",
    " \t  \ttweet.detail.2022<-rbind(tweet.detail.2022, temp2)\n",
    "\t\ttweet.detail.2022<-rbind(tweet.detail.2022, temp3)\n",
    " \t  \trm(temp1, temp2, temp3, tweets, tweets1, tweets2, tweets3)\n",
    "\t\t}\n",
    "\telse {\n",
    "    \ttemp = data.frame(action = file.nm[i], id = tweets$id, author_id = tweets$author_id, conversation_id = tweets$conversation_id, datetime = tweets$created_at, text = tweets$text, lang = tweets$lang)\n",
    "\t\tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets$text, ignore.case = TRUE))\n",
    "\t\ttemp$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t\ttemp$annotation_type = unlist(lapply(tweets$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t \ttemp$annotation_normalized_text = unlist(lapply(tweets$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp = cbind(temp, tweets$public_metrics)\n",
    "\t  \n",
    "\t  \ttweet.detail.2022 = rbind(tweet.detail.2022, temp)\n",
    "\t  \trm(temp, tweets)\n",
    "\t\t}\n",
    "\tgc()\n",
    "\tprint(i)\n",
    "\tflush.console()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(files[209])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(files[223]) #zorbing 1+2\n",
    "tweets1 = tweets[[1]]\n",
    "temp1 = data.frame(action = file.nm[i], id = tweets1$id, author_id = tweets1$author_id, conversation_id = tweets1$conversation_id, datetime = tweets1$created_at, text = tweets1$text, lang = tweets1$lang)\n",
    "feat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets1$text, ignore.case = TRUE))\n",
    "temp1$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "temp1$annotation_type = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "temp1$annotation_context = unlist(lapply(tweets1$context$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "temp1$annotation_normalized_text = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "temp1 = cbind(temp1, tweets1$public_metrics)\n",
    "\n",
    "tweet.detail.2022<-rbind(tweet.detail.2022, temp1)\n",
    "\n",
    "tweets2 = tweets[[2]]\n",
    "temp2 = data.frame(action = file.nm[i], id = tweets2$id, author_id = tweets2$author_id, conversation_id = tweets2$conversation_id, datetime = tweets2$created_at, text = tweets2$text, lang = tweets2$lang)\n",
    "feat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets2$text, ignore.case = TRUE))\n",
    "temp2$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "temp2$annotation_type = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "temp2$annotation_normalized_text = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "temp2 = cbind(temp2, tweets2$public_metrics)\n",
    "tweet.detail.2022<-rbind(tweet.detail.2022, temp2)\n",
    "\n",
    "rm(temp1, temp2, tweets, tweets1, tweets2)\n",
    "\n",
    "gc()\n",
    "print(i)\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "save(tweet.detail.2022, file = 'C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/Data/CES_tweet_details_2022_jan_sep.Rdata')\n",
    "write.csv(tweet.detail.2022, 'C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/Data/CES_tweet_details_2022_jan_sep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2020 + 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "files <- list.files(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map/2021final/\", full.names=TRUE)\n",
    "file.nm <- list.files(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map/2021final/\")\n",
    "file.nm <- gsub(\".Rdata\",\"\",file.nm)\n",
    "file.nm <- gsub(\"\\\\d\",\"\",file.nm)\n",
    "\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.actions_Outdoors_reddit.Rdata\")\n",
    "load(\"C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES.asset_EarthPorn_IUCN_reddit.Rdata\")\n",
    "\n",
    "CES.features<-CES.asset[1:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "load(files[i])\n",
    "str(tweets[[1]], vec.len = 0)\n",
    "\n",
    "#tweets = tweets[1:50,]\n",
    "\n",
    "#Create data frame with select columns\n",
    "tweet.detail.2021 = data.frame(action = file.nm[i], id = tweets$id, author_id = tweets$author_id, conversation_id = tweets$conversation_id, datetime = tweets$created_at, text = tweets$text, lang = tweets$lang)\n",
    "\n",
    "#Add feature column\n",
    "feat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets$text, ignore.case = TRUE))\n",
    "tweet.detail.2021$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "tweet.detail.2021$annotation_type = unlist(lapply(tweets$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "tweet.detail.2021$annotation_normalized_text = unlist(lapply(tweets$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "tweet.detail.2021<-cbind(tweet.detail.2021, tweets$public_metrics)\n",
    "\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(files[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for (i in 103:length(files)) {\n",
    "\tload(files[i])\n",
    "\tif(length(tweets) == 2) {\n",
    "\t\ttweets1 = tweets[[1]]#[1:50,]\n",
    "\t\ttemp1 = data.frame(action = file.nm[i], id = tweets1$id, author_id = tweets1$author_id, conversation_id = tweets1$conversation_id, datetime = tweets1$created_at, text = tweets1$text, lang = tweets1$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets1$text, ignore.case = TRUE))\n",
    "\t  \ttemp1$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t \ttemp1$annotation_type = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t\ttemp1$annotation_context = unlist(lapply(tweets1$context$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1$annotation_normalized_text = unlist(lapply(tweets1$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp1 = cbind(temp1, tweets1$public_metrics)\n",
    "\t  \n",
    "\t  \ttweets2 = tweets[[2]]#[1:50,]\n",
    "\t  \ttemp2 = data.frame(action = file.nm[i], id = tweets2$id, author_id = tweets2$author_id, conversation_id = tweets2$conversation_id, datetime = tweets2$created_at, text = tweets2$text, lang = tweets2$lang)\n",
    "\t  \tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets2$text, ignore.case = TRUE))\n",
    "\t  \ttemp2$feature = apply(feat.found,1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t  \ttemp2$annotation_type = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2$annotation_normalized_text = unlist(lapply(tweets2$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp2 = cbind(temp2, tweets2$public_metrics)\n",
    "\t  \n",
    "\t  \ttweet.detail.2021<-rbind(tweet.detail.2021, temp1)\n",
    " \t  \ttweet.detail.2021<-rbind(tweet.detail.2021, temp2)\n",
    " \t  \trm(temp1, temp2, tweets, tweets1, tweets2)\n",
    "\t\t}\n",
    "\telse {\n",
    "    \ttemp = data.frame(action = file.nm[i], id = tweets$id, author_id = tweets$author_id, conversation_id = tweets$conversation_id, datetime = tweets$created_at, text = tweets$text, lang = tweets$lang)\n",
    "\t\tfeat.found = apply(as.data.frame(CES.features), 1, function(x) grepl(x, tweets$text, ignore.case = TRUE))\n",
    "\t\ttemp$feature = apply(feat.found, 1, function(x) paste(CES.features[x],collapse = \"|\"))\n",
    "\t\ttemp$annotation_type = unlist(lapply(tweets$entities$annotations, function(x) paste(x$type[x$probability>.5], collapse = \"|\")))\n",
    "\t \ttemp$annotation_normalized_text = unlist(lapply(tweets$entities$annotations, function(x) paste(x$normalized_text[x$probability>.5], collapse = \"|\")))\n",
    "\t  \ttemp = cbind(temp, tweets$public_metrics)\n",
    "\t  \n",
    "\t  \ttweet.detail.2021 = rbind(tweet.detail.2021, temp)\n",
    "\t  \trm(temp, tweets)\n",
    "\t\t}\n",
    "\tgc()\n",
    "\tprint(i)\n",
    "\tflush.console()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "save(tweet.detail.2021, file = 'C:/Users/acali/OneDrive - Danmarks Tekniske Universitet/CES conversation map ACL/CES_tweet_details_2021.Rdata')\n",
    "write.csv(tweet.detail.2021, 'CES_tweet_details_2021.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "654f3540e6af9084947d0df77248ac19dd4be50e477a3459f2fef29575e02c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
