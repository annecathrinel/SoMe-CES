{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# specify year for the tweets dataframe you are working with\n",
    "year = '2019'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets data frame and do initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_details_filename = 'CES_tweet_details_' + year + '.csv'\n",
    "tweet_details = pd.read_csv(tweet_details_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = ['acting', 'adventuring', 'ambling', 'ascending', 'backpacking', 'baitcasting', 'balancing', 'ballooning', 'bathing', 'bikepacking', 'biking', 'birding', 'boating', 'bouldering', 'boxing', 'brewing', 'bushwalking', 'camping', 'canoeing', 'canyoneering', 'canyoning', 'casting', 'caving', 'chatting', 'chilling', 'cleaning', 'climbing', 'contemplating', 'cooking', 'crabbing', 'crawling', 'creating', 'cruising', 'cycling', 'dancing', 'daydreaming', 'descending', 'designing', 'discovering', 'diving', 'documenting', 'dogsledding', 'drawing', 'dreaming', 'drifting', 'drinking', 'driving', 'eating', 'exercising', 'exploring', 'farming', 'feasting', 'filming', 'fishing', 'flexing', 'floating', 'flyfishing', 'flying', 'foraging', 'fording', 'freediving', 'galloping', 'gardening', 'geocaching', 'glamping', 'glissading', 'golfing', 'grilling', 'guiding', 'hammocking', 'hauling', 'highlining', 'hiking', 'hitchhiking', 'humming', 'hunting', 'interacting', 'jigging', 'jogging', 'kayaking', 'kiteboarding', 'kitesurfing', 'kiting', 'landscaping', 'laying', 'learning', 'littering', 'logging', 'longboarding', 'looming', 'lounging', 'magnetfishing', 'mapping', 'marching', 'meditating', 'monitoring', 'mountaineering' 'napping', 'navigating', 'netting', 'observing', 'offroading', 'outdooring', 'paddleboarding' 'paddling', 'paintballing', 'painting', 'paragliding', 'parasailing', 'performing', 'photographing', 'pioneering', 'pitching', 'planting', 'playing', 'portaging', 'postholing', 'praying', 'pruning', 'racing', 'rafting', 'railbiking', 'rambling', 'rappelling', 'reading', 'recording', 'recycling', 'reflecting', 'relaxing', 'remembering', 'reminiscing', 'resting', 'riding', 'roadtripping', 'roaming', 'rockhounding', 'rollerblading', 'rowing', 'running', 'rving', 'sailing', 'sandboarding', 'scaling', 'scalloping', 'scootering', 'scootertuning', 'scoping', 'scouting', 'scrambling', 'searching', 'shoeboarding', 'shooting', 'shrimping', 'sightseeing', 'singing', 'siting', 'sitting', 'skateboarding', 'skating', 'sketching', 'skiing', 'skimboarding', 'skimming', 'skitouring', 'skydiving', 'slacklining', 'sledding', 'sleeping', 'snorkeling', 'snorkelling', 'snowboarding', 'snowmobiling', 'snowshoeing', 'soaking', 'solohiking', 'sparring', 'spearfishing', 'spelunking', 'splitboarding', 'sporting', 'standing', 'stargazing', 'stretching', 'strolling', 'studying', 'sunbathing', 'sunning', 'surfing', 'surveying', 'swimming', 'talking', 'teaching', 'tenting', 'thinking', 'thruhiking', 'touring', 'tracking', 'training', 'traveling', 'travelling', 'traversing', 'treking', 'trekking', 'trudging', 'tubing', 'tumbling', 'unicycling', 'vacationing', 'vandwelling', 'vibing', 'vlogging', 'volunteering', 'wading', 'walking', 'wandering', 'watering', 'weaving', 'whistling', 'wildcamping', 'working', 'writing', 'ziplining', 'zorbing']\n",
    "\n",
    "tweet_details = tweet_details.drop(['action'], axis=1)\n",
    "\n",
    "tweet_details['action'] = tweet_details['text'].str.extract(f\"({'|'.join(activity_list)})\", re.IGNORECASE)\n",
    "\n",
    "tweet_details['action'] = tweet_details['action'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet = tweet_details.sample(frac=0.01)\n",
    "tweets_copy = tweet_details.copy()\n",
    "tweets = tweets_copy.assign(feature = tweets_copy.feature.str.split('|')).explode('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non english tweets and feature = NA\n",
    "tweets = tweets[(\n",
    "    (tweets.lang == 'en') &\n",
    "    (tweets.feature.isna() == False) &\n",
    "    (tweets.action.isna() == False)\n",
    "    )]\n",
    "\n",
    "#keep features not describing places/nature types\n",
    "tweets = tweets[(\n",
    "    (tweets.feature != 'fall')  & \n",
    "    (tweets.feature != 'flower')  & \n",
    "    (tweets.feature != 'foliage')  & \n",
    "    (tweets.feature != 'grass')  & \n",
    "    (tweets.feature != 'lava')  &\n",
    "    (tweets.feature != 'moss')  & \n",
    "    (tweets.feature != 'mushroom')  & \n",
    "    (tweets.feature != 'rock')  & \n",
    "    (tweets.feature != 'sand')  & \n",
    "    (tweets.feature != 'tide')  & \n",
    "    (tweets.feature != 'tree')  & \n",
    "    (tweets.feature != 'wave') & \n",
    "    (tweets.feature != 'wildflower')\n",
    "    )]\n",
    "\n",
    "#remove actions that are not decribing nature-activity relation\n",
    "tweets = tweets[(\n",
    "    (tweets.action != 'acting')\n",
    "    )]\n",
    "\n",
    "# remove all tweets where feature == stream and action == acting and when stream refering to videos and not water streams\n",
    "tweets = tweets.loc[~(\n",
    "    ((tweets['feature'] == 'stream') & (tweets['action'] == 'acting')) |\n",
    "    ((tweets['feature'] == 'stream') & \n",
    "    (tweets['text'].str.contains('spotify|apple|soundcloud|tune in|online|movie|discord|video|drawing stream|internet|mainstream|twitch|live stream|live-stream|minecraft|streaming|raid|youtube|netflix|live')))\n",
    "    ),:]\n",
    "\n",
    "#remove bathing suit\n",
    "tweets = tweets.loc[~((tweets['action'] == 'bathing') & tweets['text'].str.contains('bathing suit', case = False)),:]\n",
    "\n",
    "#remove boxing day\n",
    "tweets = tweets.loc[~((tweets['action'] == 'soaking') & tweets['text'].str.contains('soaking wet', case = False)),:]\n",
    "\n",
    "#remove boxing day\n",
    "tweets = tweets.loc[~((tweets['action'] == 'boxing') & tweets['text'].str.contains('boxing day', case = False)),:]\n",
    "\n",
    "#remove casting in reference to voting or casting actors\n",
    "tweets = tweets.loc[~((tweets['action'] == 'casting') & tweets['text'].str.contains('voting|vote|actor|movie|ballot', case = False)),:]\n",
    "\n",
    "#remove park misclassifications\n",
    "tweets = tweets.loc[~((tweets['feature'] == 'park') & tweets['text'].str.contains('jurassic park|parking|amusement park', case = False)),:]\n",
    "\n",
    "#remove horoscope quotes\n",
    "tweets = tweets.loc[~(tweets['text'].str.contains('more for aries|more for taurus|more for gemini|more for cancer|more for leo|more for virgo|more for libra|more for scorpio|more for sagittarius|more for capricorn|more for aquarius|more for pisces', case = False)),:]\n",
    "\n",
    "#remove news tweets about mass shooting\n",
    "tweets = tweets.loc[~((tweets['action'] == 'shooting') & tweets['text'].str.contains('active shooting|murder|suspect|reported shooting|gunman|gun man|victims|mass shooting|school shooting|park shooting|police|fbi|crime|news|shooting star', case = False)),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save tweets after initial clean and a reduced data frame to annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filename = 'CES_tweets_initial_clean' + year + '.csv'\n",
    "tweets.to_csv(tweets_filename, header=True, index=False, columns=list(tweets.axes[1]))\n",
    "\n",
    "tweets_annotate = pd.DataFrame()\n",
    "tweets_annotate['text'] = tweets['text']\n",
    "tweets_annotate['id'] = tweets['id']\n",
    "\n",
    "tweets_to_annotate_filename = 'CES_tweets_to_annotate' + year + '.csv'\n",
    "tweets_annotate.to_csv(tweets_to_annotate_filename, header=True, index=False, columns=list(tweets_annotate.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Twitter context annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load annotations csv and create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_annotate = pd.read_csv('evergreen-context-entities-20220601.csv') #get csv from https://github.com/twitterdev/twitter-context-annotations/tree/main/files\n",
    "\n",
    "annotate_dict = {\n",
    "    '3'   : 'TV Shows',\n",
    "    '4'   : 'TV Episodes',\n",
    "    '6'   : 'Sports Events',\n",
    "    '10'  : 'Person',\n",
    "    '11'  : 'Sport',\n",
    "    '12'  : 'Sports Team',\n",
    "    '13'  : 'Place',\n",
    "    '22'  : 'TV Genres',\n",
    "    '23'  : 'TV Channels',\n",
    "    '26'  : 'Sports League',\n",
    "    '27'  : 'American Football Game',\n",
    "    '28'  : 'NFL Football Game',\n",
    "    '29'  : 'Events',\n",
    "    '31'  : 'Community',\n",
    "    '35'  : 'Politicians',\n",
    "    '38'  : 'Political Race',\n",
    "    '39'  : 'Basketball Game',\n",
    "    '40'  : 'Sports Series',\n",
    "    '43'  : 'Soccer Match',\n",
    "    '44'  : 'Baseball Game',\n",
    "    '45'  : 'Brand Vertical',\n",
    "    '46'  : 'Brand Category',\n",
    "    '47'  : 'Brand',\n",
    "    '48'  : 'Product',\n",
    "    '54'  : 'Musician',\n",
    "    '55'  : 'Music Genre',\n",
    "    '56'  : 'Actor',\n",
    "    '58'  : 'Entertainment Personality',\n",
    "    '60'  : 'Athlete',\n",
    "    '65'  : 'Interests and Hobbies Vertical', #\n",
    "    '66'  : 'Interests and Hobbies Category', #\n",
    "    '67'  : 'Interests and Hobbies', #\n",
    "    '68'  : 'Hockey Game',\n",
    "    '71'  : 'Video Game',\n",
    "    '78'  : 'Video Game Publisher',\n",
    "    '79'  : 'Video Game Hardware',\n",
    "    '83'  : 'Cricket Match',\n",
    "    '84'  : 'Book',\n",
    "    '85'  : 'Book Genre',\n",
    "    '86'  : 'Movie',\n",
    "    '87'  : 'Movie Genre',\n",
    "    '88'  : 'Political Body',\n",
    "    '89'  : 'Music Album',\n",
    "    '90'  : 'Radio Station',\n",
    "    '91'  : 'Podcast',\n",
    "    '92'  : 'Sports Personality',\n",
    "    '93'  : 'Coach',\n",
    "    '94'  : 'Journalist',\n",
    "    '95'  : 'TV Channel [Entity Service]',\n",
    "    '109' : 'Reoccurring Trends', #\n",
    "    '110' : 'Viral Accounts',\n",
    "    '114' : 'Concert',\n",
    "    '115' : 'Video Game Conference',\n",
    "    '116' : 'Video Game Tournament',\n",
    "    '117' : 'Movie Festival',\n",
    "    '118' : 'Award Show',\n",
    "    '119' : 'Holiday',\n",
    "    '120' : 'Digital Creator',\n",
    "    '122' : 'Fictional Character',\n",
    "    '130' : 'Multimedia Franchise',\n",
    "    '131' : 'Unified Twitter Taxonomy',\n",
    "    '136' : 'Video Game Personality',\n",
    "    '137' :' eSports Team',\n",
    "    '138' : 'eSports Player',\n",
    "    '139' : 'Fan Community',\n",
    "    '149' : 'Esports League',\n",
    "    '152' : 'Food', #\n",
    "    '155' : 'Weather', #\n",
    "    '156' : 'Cities', #\n",
    "    '157' : 'Universities',\n",
    "    '158' : 'Points of Interest',\n",
    "    '159' : 'States', #\n",
    "    '160' : 'Countries', #\n",
    "    '162' : 'Exercise & fitness',\n",
    "    '163' : 'Travel', #\n",
    "    '164' : 'Fields of study',\n",
    "    '165' : 'Technology',\n",
    "    '166' : 'Stocks',\n",
    "    '167' : 'Animals', # -> do not remove --> bernese mountain dog\n",
    "    '171' : 'Local News',\n",
    "    '172' : 'Global TV Show',\n",
    "    '173' : 'Google Product Taxonomy',\n",
    "    '174' : 'Digital Assets & Crypto', #\n",
    "    '175' : 'Emergency Events'\n",
    "}\n",
    "\n",
    "context_annotate['domains_list'] = context_annotate['domains'].str.split(',')\n",
    "context_annotate['context_annotation'] = context_annotate['domains_list'].apply(lambda x: [annotate_dict.get(item) for item in x])\n",
    "context_annotate['entity_name'].replace(to_replace = '\\\\t+', value = '', regex = True, inplace = True)\n",
    "\n",
    "#searchfor = context_annotate['entity_name'].str.lower().tolist()\n",
    "#tweet_sample_2021['text'] = tweet_sample_2021['text'].str.lower()\n",
    "\n",
    "#remove all non ASCII columns --> removes approx. 6500 rows\n",
    "context_annotate = context_annotate.loc[~(context_annotate['entity_name'].str.contains(r'[^\\x00-\\x7F]+')),:]\n",
    "context_annotate = context_annotate.loc[~(context_annotate['entity_name'].str.contains('community-')),:]\n",
    "\n",
    "context_annotate_dict_filename = 'context_annotations_dict.csv'\n",
    "context_annotate.to_csv(context_annotate_dict_filename, header=True, index=False, columns=list(context_annotate.axes[1]))\n",
    "\n",
    "#searchfor = context_annotate['entity_name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add context annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_annotate_filename = 'CES_tweets_to_annotate_' + year + '.csv'\n",
    "tweets = pd.read_csv(tweets_to_annotate_filename)\n",
    "\n",
    "context_annotate_dict_filename = 'context_annotations_dict.csv'\n",
    "context_annotate = pd.read_csv(context_annotate_dict_filename)\n",
    "\n",
    "tweets['context_match'] = ''\n",
    "\n",
    "searchfor = context_annotate['entity_name'].tolist()\n",
    "\n",
    "for name in searchfor:\n",
    "    tweets.loc[tweets['text'].str.contains(name, case = False, na = False, regex = False), 'context_match'] += '|' + name\n",
    "\n",
    "annotated_tweets_filename = 'CES_annotated_tweets_' + year + '.csv'\n",
    "tweets.to_csv(annotated_tweets_filename, header=True, index=False, columns=list(tweets.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join outputs from context annotations with full tweet details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filename = 'CES_tweets_initial_clean' + year + '.csv'\n",
    "tweets = pd.read_csv(tweets_filename)\n",
    "\n",
    "annotated_tweets_filename = 'CES_annotated_tweets_' + year + '.csv'\n",
    "annotated_tweets = pd.read_csv(annotated_tweets_filename)\n",
    "\n",
    "annotated_tweets.dropna(subset=['id'], inplace = True)\n",
    "\n",
    "tweets['context_entities'] = annotated_tweets['entities'].tolist()\n",
    "tweets['context_entities'] = tweets['context_entities'].str.replace('[' , '', regex = False)\n",
    "tweets['context_entities'] = tweets['context_entities'].str.replace(']' , '', regex = False)\n",
    "tweets['context_entities'] = tweets['context_entities'].str.replace('\"' , '', regex = False)\n",
    "tweets['context_entities'] = tweets['context_entities'].str.replace(\"'\" , '', regex = False)\n",
    "tweets['context_entities'] = tweets['context_entities'].str.split(',')\n",
    "tweets_explode = tweets.explode('context_entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_annotate_dict_filename = 'context_annotations_dict.csv'\n",
    "context_annotate = pd.read_csv(context_annotate_dict_filename)\n",
    "\n",
    "context_annotate['context_annotation'] = context_annotate['context_annotation'].str.replace('[' , '', regex = False)\n",
    "context_annotate['context_annotation'] = context_annotate['context_annotation'].str.replace(']' , '', regex = False)\n",
    "context_annotate['context_annotation'] = context_annotate['context_annotation'].str.replace('\"' , '', regex = False)\n",
    "context_annotate['context_annotation'] = context_annotate['context_annotation'].str.replace(\"'\" , '', regex = False)\n",
    "context_annotate['context_annotation'] = context_annotate['context_annotation'].str.replace(', ' , '|', regex = False)\n",
    "\n",
    "context_dict = dict(zip(context_annotate.entity_name, context_annotate.context_annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_explode['context_annotation'] = (tweets_explode['context_entities'].apply(lambda x: [context_dict.get(item) for item in x]))\n",
    "\n",
    "tweets_explode['context_annotation'] = tweets_explode['context_annotation'].apply(lambda x: ','.join(map(str, x)))\n",
    "tweets_explode['context_annotation'] = tweets_explode['context_annotation'].str.replace('None' , '', regex = False)\n",
    "tweets_explode['context_annotation'] = tweets_explode['context_annotation'].str.replace(',' , '', regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_explode['context_entities'] = tweets_explode['context_entities'].str.lower()\n",
    "tweets_explode['ent_is_feat'] = tweets_explode.apply(lambda x: 'Yes' if x['feature'] in x['context_entities'] else 'No', axis=1)\n",
    "tweets_explode['ent_is_act'] = tweets_explode.apply(lambda x: 'Yes' if x['action'] in x['context_entities'] else 'No', axis=1)\n",
    "\n",
    "condition_feat = ((tweets_explode['ent_is_feat'] == 'Yes') & (tweets_explode['context_annotation'] != 'Place') & (tweets_explode['context_annotation'] != ''))\n",
    "condition_act = ((tweets_explode['ent_is_act'] == 'Yes') & (tweets_explode['context_annotation'] != ''))\n",
    "\n",
    "tweets_explode['remove_feat'] = np.where(condition_feat, 'Yes', '')\n",
    "tweets_explode['remove_act'] = np.where(condition_act, 'Yes', '')\n",
    "\n",
    "tweets_annotated = tweets_explode.groupb(['action', 'id', 'feature'], as_index = False).agg({'author_id' : 'first', 'conversation_id' : 'first', 'datetime' : 'first', 'text' : 'first', 'annotation_type' : 'first', 'annotation_normalized_text' : 'first', 'context_entities' : ','.join, 'context_annotation' : ','.join, 'remove_feat' : ' '.join, 'remove_act' : ' '.join})\n",
    "\n",
    "tweets_remove = tweets_annotated.loc[~((tweets_annotated['remove_feat'].str.contains('Yes', case = False)) | (tweets_annotated['remove_act'].str.contains('Yes', case = False))),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_explode, tweets_annotated, condition_feat, condition_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_remove['annotation_normalized_text'] = tweets_remove['annotation_normalized_text'] .fillna('')\n",
    "tweets_remove['annotation_type'] = tweets_remove['annotation_type'] .fillna('')\n",
    "\n",
    "tweets_remove['ent_is_feat'] = tweets_remove.apply(lambda x: 'Yes' if x['feature'] in x['annotation_normalized_text'] else 'No', axis=1)\n",
    "tweets_remove['ent_is_act'] = tweets_remove.apply(lambda x: 'Yes' if x['action'] in x['annotation_normalized_text'] else 'No', axis=1)\n",
    "\n",
    "condition_feat = ((tweets_remove['ent_is_feat'] == 'Yes') & (tweets_remove['annotation_type'] != 'Place') & (tweets_remove['annotation_type'] != ''))\n",
    "condition_act = ((tweets_remove['ent_is_act'] == 'Yes') & (tweets_remove['annotation_type'] != ''))\n",
    "\n",
    "tweets_remove['remove_feat'] = np.where(condition_feat, 'Yes', '')\n",
    "tweets_remove['remove_act'] = np.where(condition_act, 'Yes', '')\n",
    "\n",
    "tweets_remove = tweets_remove.loc[~((tweets_remove['remove_feat'].str.contains('Yes', case = False)) | (tweets_remove['remove_act'].str.contains('Yes', case = False))),:]\n",
    "\n",
    "tweets_annotated = tweets_remove.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_remove, condition_feat, condition_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_annotated = tweets_annotated.drop(['remove_act', 'remove_feat', 'ent_is_feat', 'ent_is_act'], axis=1)\n",
    "\n",
    "annotated_tweets_filename = 'CES_tweets_annotations_cleaned' + year + '.csv'\n",
    "tweets_annotated.to_csv(annotated_tweets_filename, header=True, index=False, columns=list(tweets_annotated.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create summarized data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sum = tweets_annotated.groupby(['action', 'feature', 'date'])['text'].count().reset_index(name='tweets')\n",
    "\n",
    "tweets_sum['date'] = pd.to_datetime(tweets_sum['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sum_filename = 'tweets_sum' + year + '.csv'\n",
    "tweets_sum.to_csv(tweets_sum_filename, header=True, index=False, columns=list(tweets_sum.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tweet counts for cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_details_filename = 'CES_tweets_details.' + year + 'csv'\n",
    "tweets_details = pd.read_csv(tweets_details_filename)\n",
    "tweets_original = tweets_details.groupby(['id'], as_index = False).agg({'text' : 'first'})\n",
    "\n",
    "annotation_tweets_filename = 'CES_tweets_to_annotate' + year + '.csv'\n",
    "annotation_tweets = pd.read_csv(annotation_tweets_filename)\n",
    "tweets_initial_clean = annotation_tweets.groupby(['id'], as_index = False).agg({'text' : 'first'})\n",
    "\n",
    "tweets_cleaned_filename = 'CES_tweets_annotations_cleaned.' + year + 'csv'\n",
    "tweets_cleaned = pd.read_csv(tweets_cleaned_filename)\n",
    "tweets_annotations_cleaned = tweets_cleaned.groupby(['id'], as_index = False).agg({'text' : 'first'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "654f3540e6af9084947d0df77248ac19dd4be50e477a3459f2fef29575e02c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
